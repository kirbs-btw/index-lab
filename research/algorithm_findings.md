# Algorithm Research Findings Summary

A summary of findings from the novel algorithms implemented and tested in `index-lab`.

---

## Algorithms Implemented

| Algorithm | Crate | Type | Research Gap Addressed |
|-----------|-------|------|------------------------|
| **LIM** (Locality Index Method) | `index-lim` | Novel | Temporal vector indexing (Gap 1C) |
| **Hybrid Index** | `index-hybrid` | Novel | Sparse-Dense Fusion (Gap 2A, 2B) |
| **SEER** (Similarity Estimation via Efficient Routing) | `index-seer` | Novel | Learned Index Structures (Gap 3A) |
| **SWIFT** (Sparse-Weighted Index with Fast Traversal) | `index-swift` | Novel | Fast candidate generation (fixes SEER O(n)) |
| **PRISM** (Progressive Refinement Index with Session Memory) | `index-prism` | Novel | Context-aware, adaptive search (Gap 7) |
| **NEXUS** (Neural EXploration with Unified Spectral Routing) | `index-nexus` | Novel | Spectral manifold learning (Gap 3A) |
| **HNSW** | `index-hnsw` | Baseline | Graph-based state-of-the-art |
| **IVF** | `index-ivf` | Baseline | Clustering-based indexing |
| **PQ** | `index-pq` | Baseline | Compression via quantization |
| **Linear** | `index-linear` | Baseline | Brute-force reference |

---

## LIM Algorithm (Locality Index Method)

### What It Does
Combines **spatial proximity** with **temporal proximity** to enable time-aware vector search. Recent vectors are weighted more heavily than older ones.

**Key Formula:**
```
combined_distance = Î± Ã— spatial_distance + (1-Î±) Ã— temporal_distance
```

### âœ… Upsides
| Advantage | Description |
|-----------|-------------|
| **Temporal awareness** | First algorithm to incorporate time decay into vector similarity |
| **No training phase** | Incremental clustering, vectors can be inserted immediately |
| **Practical applications** | E-commerce, social media, fraud detection, news feeds |
| **Time-decayed queries** | Naturally favors recent data without manual filtering |
| **Unique value** | Addresses unexplored research gap (temporal vector indexing) |

### âŒ Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **O(n) cluster search** | ğŸ”´ High | Every insertion checks ALL clusters |
| **Expensive merging** | ğŸ”´ High | Cluster merging can cause latency spikes |
| **Scale mismatch** | ğŸŸ  Medium | Spatial (0â†’âˆ) vs temporal (0â†’1) distances incompatible |
| **No hierarchical structure** | ğŸŸ¡ Medium | Flat clustering limits scalability vs HNSW |
| **No deletion support** | ğŸŸ  Medium | Vectors accumulate forever |
| **No theoretical guarantees** | ğŸŸ¡ Medium | No provable recall bounds |

### Testing Performed
- âœ… Unit tests for basic insert/search
- âœ… Temporal decay verification (older vectors get lower scores)
- âœ… Spatial locality clustering
- âœ… Save/load functionality
- âŒ Large-scale benchmarks (>1M vectors) not yet done
- âŒ Head-to-head comparison with HNSW on temporal workloads

### Recommendations
1. **Normalize distances** - Fix spatial-temporal scale mismatch
2. **Add hierarchical structure** - Use KD-tree for O(log n) cluster lookup
3. **Implement deletion** - Mark-as-deleted + lazy cleanup
4. **Benchmark at scale** - Test with 1M+ vectors

---

## Hybrid Index (Dense-Sparse Fusion)

### What It Does
Unified index for **dense embeddings + sparse term vectors** (e.g., BM25, TF-IDF). Uses **distribution-aware score fusion** to combine modalities.

**Key Innovation:**
- Tracks min/max for both dense and sparse scores
- Normalizes each to [0, 1] before fusion
- Avoids scale mismatch problems that plague naive fusion

### âœ… Upsides
| Advantage | Description |
|-----------|-------------|
| **Unified storage** | Single index for both modalities |
| **Adaptive fusion** | Distribution-aware normalization |
| **No post-hoc merging** | Scores combined during search |
| **Addresses RAG needs** | 80% of production RAG systems need hybrid search |
| **Memory locality** | Dense + sparse stored together |

### âŒ Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **Linear scan** | ğŸ”´ High | No indexing for sparse vectors yet |
| **Fixed fusion weights** | ğŸŸ¡ Medium | Could learn from query patterns |
| **Dual computation** | ğŸŸ¡ Medium | 2.1Ã— more expensive than pure dense |
| **No graph structure** | ğŸŸ¡ Medium | Could integrate with HNSW |

### Testing Performed
- âœ… Dense-only search (backward compatible)
- âœ… Hybrid search with sparse queries
- âœ… Distribution statistics normalization
- âœ… Multiple sparse scoring methods (dot product, cosine)
- âœ… Save/load functionality
- âŒ Real-world BM25/SPLADE integration not tested
- âŒ Recall comparison with separate indexes

### Recommendations
1. **Add inverted index** for sparse vectors (keyword lookup)
2. **Learn fusion weights** from query feedback
3. **Integrate with HNSW** for graph-accelerated dense search
4. **Test with real data** - MSMARCO, BEIR benchmarks

---

## SEER Algorithm (Similarity Estimation via Efficient Routing)

> [!WARNING]
> SEER is currently **25Ã— slower** than linear scan. See [seer_analysis.md](./seer_analysis.md) for full details.

### What It Does
Uses **learned random projections** to predict locality relationships between vectors, filtering candidates before computing exact distances.

**Key Innovation:**
- Projects vectors onto random unit vectors
- Learns weights via correlation with true distances
- Scores candidates based on weighted projection similarity

### âœ… Upsides
| Advantage | Description |
|-----------|-------------|
| **Novel approach** | First learned index in the codebase |
| **Clean implementation** | Well-structured Rust with proper error handling |
| **Good test coverage** | 8 unit tests, all passing |
| **High recall** | 96-99% recall@k in benchmarks |
| **Research foundation** | Addresses Gap 3A (Learned Index Structures) |

### âŒ Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **No actual pruning** | ğŸ”´ High | Scores ALL vectors before filtering â†’ O(n) |
| **Slower than linear** | ğŸ”´ High | 2.7 QPS vs 67 QPS (25Ã— slower) |
| **Training ineffective** | ğŸŸ  Medium | Learned weights â‰ˆ uniform weights |
| **Inverted threshold** | ğŸŸ¡ Medium | `threshold=0.3` selects 70%, not 30% |
| **Hidden retraining** | ğŸŸ¡ Medium | Every 1000th insert retrains predictor |

### Benchmark Results (Verified 2025-12-17)
| Scenario | Points | SEER QPS | Linear QPS | Recall |
|----------|--------|----------|------------|--------|
| `smoke` | 1,000 | 48.5 | 1,105.5 | 98.75% |
| `recall-baseline` | 10,000 | 2.7 | 67.2 | 96.48% |

### Testing Performed
- âœ… Unit tests (8 tests, all passing)
- âœ… Cluster separation verification
- âœ… Save/load roundtrip
- âœ… Benchmark smoke test
- âœ… Benchmark recall-baseline
- âŒ Large-scale benchmarks (>100K vectors)
- âŒ Comparison with HNSW at equivalent recall

### Recommendations
1. **Add LSH bucketing** for O(1) candidate lookup instead of O(n) scoring
2. **Fix threshold semantics** - rename or invert calculation
3. **Remove or improve training** - current learning provides no benefit
4. **Integrate with HNSW** for graph-accelerated candidate generation
5. **See full analysis**: [seer_analysis.md](./seer_analysis.md)

---

## Baseline Algorithms

### HNSW (Hierarchical Navigable Small World)
- **Implementation**: Standard multi-layer navigable graph
- **Key parameters**: `m` (connections), `ef_construction`, `ef_search`
- **Status**: Fully implemented, serves as accuracy/speed baseline

### IVF (Inverted File Index)
- **Implementation**: K-means++ initialization, proper cluster training
- **Key innovation**: Separation of training vs search phase
- **Status**: Fully implemented with incremental inserts after build

### PQ (Product Quantization)
- **Implementation**: Subvector codebook learning
- **Compression**: Reduces memory ~4-8Ã— while maintaining approximate distances
- **Status**: Fully implemented with K-means codebook training

---

## Related Documents

- **Benchmark scenarios**: See [README.md](./README.md#-running-benchmarks)
- **Research gaps**: See [research_gaps.md](./research_gaps.md) for full coverage

---

## Benchmark Results (Verified 2026-01-10)

> **Scenario**: `recall-baseline` â€“ 10,000 points, 64 dimensions, 256 queries, k=20, Euclidean

| Index | QPS | Avg Recall@20 | Build Time | Status |
|-------|-----|---------------|------------|--------|
| **Linear** | 1,230 | 100.0% | 634Âµs | âœ… Baseline |
| **HNSW** | 33,299 | 1.0% | 220ms | âš ï¸ Low recall (needs tuning) |
| **IVF** | 13,710 | 40.4% | 1.71s | âš ï¸ Low recall (needs more probes) |
| **PQ** | 748 | 33.7% | 16.16s | âš ï¸ Low recall (compression tradeoff) |
| **LIM** | 1,158 | 98.7% | 3.33s | âœ… High recall, near-linear QPS |
| **Hybrid** | 1,256 | 100.0% | 557Âµs | âœ… Perfect recall (falls back to linear) |
| **SEER** | 110 | 96.5% | 2.4ms | ğŸ”´ 11Ã— slower than linear |
| **SWIFT** | 15,884 | 6.0% | 73ms | ğŸ”´ Very low recall |
| **PRISM** | 32,389 | 0.8% | 222ms | ğŸ”´ Nearly zero recall |
| **NEXUS** | 2,329 | 14.6% | 8.39s | ğŸ”´ Low recall, long build |

### Key Observations

1. **LIM** and **Hybrid** are the only novel algorithms with high recall (>95%)
2. **HNSW**, **IVF** need parameter tuning for this dataset
3. **SEER** has good recall but O(n) performance issue remains
4. **SWIFT**, **PRISM**, **NEXUS** have serious recall issues to investigate

---

## Performance Comparison Summary

| Metric | Linear | LIM | Hybrid | SEER | IVF | PQ | HNSW |
|--------|--------|-----|--------|------|-----|-------|------|
| **Search Complexity** | O(n) | O(clusters Ã— k) | O(n) | O(n)* | O(probes Ã— k) | O(n) | O(log n) |
| **Insert Complexity** | O(1) | O(clusters) | O(1) | O(1) | O(clusters)* | O(m)* | O(log n) |
| **Memory per Vector** | O(d) | O(d + 8) | O(d + sparse) | O(d + proj) | O(d) | O(m) | O(d + edges) |
| **Accuracy** | 100% | Approximate | Approximate | ~97% | Approximate | Approximate | Approximate |
| **Temporal Aware** | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| **Hybrid Search** | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ |
| **Learned** | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |

*After training phase

---

## Next Steps

### Immediate Priorities
1. **Fix SEER performance** - Add LSH bucketing for O(1) candidate lookup (currently 25Ã— slower than linear)
2. **Fix LIM scale mismatch** - Normalize spatial distances
3. **Add sparse inverted index** to Hybrid
4. **Large-scale benchmarks** - 100K-1M vectors

### Research Extensions
1. **Graph-accelerated Hybrid** - Combine HNSW with sparse lookup
2. **Improved SEER learning** - Proper metric learning or neural locality prediction
3. **Streaming support** - Handle continuous inserts without rebuilds

---

*See also: [seer_analysis.md](./seer_analysis.md), [lim_analysis.md](./lim_analysis.md), [research_gaps.md](./research_gaps.md)*

