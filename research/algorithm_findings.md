# Algorithm Research Findings Summary

A summary of findings from the novel algorithms implemented and tested in `index-lab`.

---

## Structure

| Section | Content |
|---------|---------|
| [Algorithms Implemented](#algorithms-implemented) | Overview table |
| [Per-algorithm deep dives](#lim-algorithm-locality-index-method) | LIM, Hybrid, SEER, SWIFT, NEXUS, PRISM, FUSION, VORTEX, ARMI |
| [Baseline Algorithms](#baseline-algorithms) | HNSW, IVF, PQ |
| [Benchmark Results](#benchmark-results-verified-2026-01-12) | QPS, recall, build time |
| [Performance Comparison](#performance-comparison-summary) | Complexity, memory, features |
| [Next Steps](#next-steps) | Priorities + research extensions |

*See also: [SOTA_SUMMARY.md](./SOTA_SUMMARY.md) for SOTA assessment.*

---

## Algorithms Implemented

| Algorithm | Crate | Type | Research Gap Addressed |
|-----------|-------|------|------------------------|
| **LIM** (Locality Index Method) | `index-lim` | Novel | Temporal vector indexing (Gap 1C) |
| **Hybrid Index** | `index-hybrid` | Novel | Sparse-Dense Fusion (Gap 2A, 2B) |
| **SEER** (Similarity Estimation via Efficient Routing) | `index-seer` | Novel | Learned Index Structures (Gap 3A) |
| **SWIFT** (Sparse-Weighted Index with Fast Traversal) | `index-swift` | Novel | Fast candidate generation (fixes SEER O(n)) |
| **PRISM** (Progressive Refinement Index with Session Memory) | `index-prism` | Novel | Context-aware, adaptive search (Gap 7) |
| **NEXUS** (Neural EXploration with Unified Spectral Routing) | `index-nexus` | Novel | Spectral manifold learning (Gap 3A) |
| **FUSION** (Fast Unified Search with Intelligent Orchestrated Navigation) | `index-fusion` | Novel | LSH + Mini-graphs (fixes O(n) issues) |
| **VORTEX** (Voronoi-Optimized Routing for Traversal) | `index-vortex` | Novel | Cluster-based graph routing (Gap 2B) |
| **ATLAS** (Adaptive Tiered Layered Aggregation System) | `index-atlas` | Novel | Learned routing + hybrid buckets (Gaps 1A, 2C, 3A, 7A) |
| **ARMI** (Adaptive Robust Multi-Modal Index) | `index-armi` | Novel | Multi-modal + robustness + adaptive tuning (Gaps 1B, 5, 6A, 6B, 7A) |
| **ZENITH** (Zero-configuration Enhanced Navigable Index) | `index-zenith` | Novel | Zero-config HNSW with auto-tuning (best benchmarked) |
| **HNSW** | `index-hnsw` | Baseline | Graph-based state-of-the-art |
| **IVF** | `index-ivf` | Baseline | Clustering-based indexing |
| **PQ** | `index-pq` | Baseline | Compression via quantization |
| **Linear** | `index-linear` | Baseline | Brute-force reference |

---

## LIM Algorithm (Locality Index Method)

### What It Does
Combines **spatial proximity** with **temporal proximity** to enable time-aware vector search. Recent vectors are weighted more heavily than older ones.

**Key Formula:**
```
combined_distance = Œ± √ó spatial_distance + (1-Œ±) √ó temporal_distance
```

### ‚úÖ Upsides
| Advantage | Description |
|-----------|-------------|
| **Temporal awareness** | First algorithm to incorporate time decay into vector similarity |
| **No training phase** | Incremental clustering, vectors can be inserted immediately |
| **Practical applications** | E-commerce, social media, fraud detection, news feeds |
| **Time-decayed queries** | Naturally favors recent data without manual filtering |
| **Unique value** | Addresses unexplored research gap (temporal vector indexing) |

### ‚ùå Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **O(n) cluster search** | üî¥ High | Every insertion checks ALL clusters |
| **Expensive merging** | üî¥ High | Cluster merging can cause latency spikes |
| **Scale mismatch** | üü† Medium | Spatial (0‚Üí‚àû) vs temporal (0‚Üí1) distances incompatible |
| **No hierarchical structure** | üü° Medium | Flat clustering limits scalability vs HNSW |
| **No deletion support** | üü† Medium | Vectors accumulate forever |
| **No theoretical guarantees** | üü° Medium | No provable recall bounds |

### Testing Performed
- ‚úÖ Unit tests for basic insert/search
- ‚úÖ Temporal decay verification (older vectors get lower scores)
- ‚úÖ Spatial locality clustering
- ‚úÖ Save/load functionality
- ‚ùå Large-scale benchmarks (>1M vectors) not yet done
- ‚ùå Head-to-head comparison with HNSW on temporal workloads

### Recommendations
1. **Normalize distances** - Fix spatial-temporal scale mismatch
2. **Add hierarchical structure** - Use KD-tree for O(log n) cluster lookup
3. **Implement deletion** - Mark-as-deleted + lazy cleanup
4. **Benchmark at scale** - Test with 1M+ vectors

---

## Hybrid Index (Dense-Sparse Fusion)

### What It Does
Unified index for **dense embeddings + sparse term vectors** (e.g., BM25, TF-IDF). Uses **distribution-aware score fusion** to combine modalities.

**Key Innovation:**
- Tracks min/max for both dense and sparse scores
- Normalizes each to [0, 1] before fusion
- Avoids scale mismatch problems that plague naive fusion

### ‚úÖ Upsides
| Advantage | Description |
|-----------|-------------|
| **Unified storage** | Single index for both modalities |
| **Adaptive fusion** | Distribution-aware normalization |
| **No post-hoc merging** | Scores combined during search |
| **Addresses RAG needs** | 80% of production RAG systems need hybrid search |
| **Memory locality** | Dense + sparse stored together |

### ‚ùå Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **Linear scan** | üî¥ High | No indexing for sparse vectors yet |
| **Fixed fusion weights** | üü° Medium | Could learn from query patterns |
| **Dual computation** | üü° Medium | 2.1√ó more expensive than pure dense |
| **No graph structure** | üü° Medium | Could integrate with HNSW |

### Testing Performed
- ‚úÖ Dense-only search (backward compatible)
- ‚úÖ Hybrid search with sparse queries
- ‚úÖ Distribution statistics normalization
- ‚úÖ Multiple sparse scoring methods (dot product, cosine)
- ‚úÖ Save/load functionality
- ‚ùå Real-world BM25/SPLADE integration not tested
- ‚ùå Recall comparison with separate indexes

### Recommendations
1. **Add inverted index** for sparse vectors (keyword lookup)
2. **Learn fusion weights** from query feedback
3. **Integrate with HNSW** for graph-accelerated dense search
4. **Test with real data** - MSMARCO, BEIR benchmarks

---

## SEER Algorithm (Similarity Estimation via Efficient Routing)

> [!WARNING]
> SEER is currently **25√ó slower** than linear scan. See [seer_analysis.md](./seer_analysis.md) for full details.

### What It Does
Uses **learned random projections** to predict locality relationships between vectors, filtering candidates before computing exact distances.

**Key Innovation:**
- Projects vectors onto random unit vectors
- Learns weights via correlation with true distances
- Scores candidates based on weighted projection similarity

### ‚úÖ Upsides
| Advantage | Description |
|-----------|-------------|
| **Novel approach** | First learned index in the codebase |
| **Clean implementation** | Well-structured Rust with proper error handling |
| **Good test coverage** | 8 unit tests, all passing |
| **High recall** | 96-99% recall@k in benchmarks |
| **Research foundation** | Addresses Gap 3A (Learned Index Structures) |

### ‚ùå Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **No actual pruning** | üî¥ High | Scores ALL vectors before filtering ‚Üí O(n) |
| **Slower than linear** | üî¥ High | 2.7 QPS vs 67 QPS (25√ó slower) |
| **Training ineffective** | üü† Medium | Learned weights ‚âà uniform weights |
| **Inverted threshold** | üü° Medium | `threshold=0.3` selects 70%, not 30% |
| **Hidden retraining** | üü° Medium | Every 1000th insert retrains predictor |

### Benchmark Results (Verified 2025-12-17)
| Scenario | Points | SEER QPS | Linear QPS | Recall |
|----------|--------|----------|------------|--------|
| `smoke` | 1,000 | 48.5 | 1,105.5 | 98.75% |
| `recall-baseline` | 10,000 | 2.7 | 67.2 | 96.48% |

### Testing Performed
- ‚úÖ Unit tests (8 tests, all passing)
- ‚úÖ Cluster separation verification
- ‚úÖ Save/load roundtrip
- ‚úÖ Benchmark smoke test
- ‚úÖ Benchmark recall-baseline
- ‚ùå Large-scale benchmarks (>100K vectors)
- ‚ùå Comparison with HNSW at equivalent recall

### Recommendations
1. **Add LSH bucketing** for O(1) candidate lookup instead of O(n) scoring
2. **Fix threshold semantics** - rename or invert calculation
3. **Remove or improve training** - current learning provides no benefit
4. **Integrate with HNSW** for graph-accelerated candidate generation
5. **See full analysis**: [seer_analysis.md](./seer_analysis.md)

---

## SWIFT (Sparse-Weighted Index with Fast Traversal)

> [!WARNING]
> SWIFT currently suffers from **extremely low recall (6.0%)** in benchmarks. See [swift_analysis.md](./swift_analysis.md) for architecture.

### What It Does
Decomposes search into three stages: **LSH bucketing** (O(1)), **Mini-graph navigation** (O(log b)), and **refinement** (O(k)). Aims to solve the O(n) bottleneck of SEER and Hybrid.

**Key Innovation:**
- Layered filtering: LSH ‚Üí Mini-Graph ‚Üí Rerank
- Multi-probe LSH to find candidate buckets
- Sparse inverted index extension for hybrid search

### ‚úÖ Upsides
| Advantage | Description |
|-----------|-------------|
| **Theoretical Speed** | O(1) candidate generation vs O(n) scan |
| **Scalable Architecture** | Mini-graphs are easier to maintain than one giant graph |
| **Hybrid & Temporal** | Designed to support multi-modal and temporal search natively |
| **Simple Components** | Composed of standard LSH and HNSW parts |

### ‚ùå Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **Catastrophic Recall** | üî¥ Critical | Currently achieves only 6.0% recall (target 95%) |
| **Bucket Imbalance** | üî¥ High | Risk of empty or mega-buckets degrading performance |
| **Dimensionality Curse** | üü° Medium | High-dim vectors require many hyperplanes for good hashing |
| **Cold Start** | üü° Medium | Requires enough data to form meaningful buckets |

### Recommendations
1. **Debug LSH Hashing** - Identify why recall is so low (hash collisions, probing depth)
2. **Implement Dynamic Buckets** - Split/merge buckets based on size
3. **Verify Mini-Graph Connectivity** - Ensure graphs within buckets are navigable
4. **Increase Probes** - Aggressively probe more buckets to improve recall

---

## NEXUS (Neural EXploration with Unified Spectral Routing)

### What It Does
Exploits **manifold structure** via spectral embedding. Projects high-dim vectors to low-dim spectral space for fast filtering, then reranks with full vectors.

**Key Innovation:**
- **Spectral Filtering**: O(m) distance vs O(d) distance (e.g., 32 vs 768 dims)
- **Adaptive Graph**: Allocates more edges to "hard" (high entropy) regions
- **Two-Phase Search**: Fast spectral search ‚Üí Accurate full search

### ‚úÖ Upsides
| Advantage | Description |
|-----------|-------------|
| **Manifold Aware** | Adaptation to data geometry via entropy estimation |
| **Fast Traversal** | Spectral distances are ~24√ó cheaper to compute |
| **Adaptive Topology** | Variable edge counts optimize for local density |
| **Novelty** | Addresses "Learned Index Structures" gap (Gap 3A) |

### ‚ùå Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **Build Complexity** | üî¥ High | O(n¬≤) entropy estimation makes build time prohibitive |
| **Low Recall** | üî¥ High | Currently 14.6% recall; random projections are too noisy |
| **Approximation Loss** | üü° Medium | Random projection is not true spectral decomposition |
| **No Neural Router** | üü° Medium | "Neural" part (NRP) not yet implemented |

### Recommendations
1. **Optimize Build** - Use approximate k-NN for entropy estimation (O(n log n))
2. **True Spectral Embedding** - Replace random projections with actual eigendecomposition
3. **Implement NRP** - Train a small MLP to route queries instead of greedy search
4. **Hierarchical Entry** - Add HNSW-like layers for faster entry point finding

---

## PRISM (Progressive Refinement Index with Session Memory)

### What It Does
Wraps an underlying index (like HNSW) with **session memory**. Caches "hot regions" and adapts search parameters (`ef`) based on query difficulty and similarity to recent history.

**Key Innovation:**
- **Context Awareness**: Remembers recent queries and results
- **Adaptive Search**: Increases `ef` for hard queries, decreases for related ones
- **Hot Region Caching**: Direct jumping to relevant graph neighborhoods

### ‚úÖ Upsides
| Advantage | Description |
|-----------|-------------|
| **Session optimization** | Speeds up related queries in a sequence |
| **Lightweight** | Minimal memory overhead (~50KB/session) |
| **Clean Design** | Wraps existing indexes without internal changes |
| **High Recall** | Maintains HNSW-level recall (~98%) |

### ‚ùå Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **Trait Limitations** | üü† Medium | `VectorIndex::search` is immutable, complicating state updates |
| **Limited "Warm Start"** | üü† Medium | Cannot pass entry points to HNSW internal search |
| **Zero Recall Bug** | üî¥ Critical | Benchmark shows 0.8% recall (likely implementation bug) |
| **State Complexity** | üü° Medium | Managing session expiry and persistence is complex |

### Recommendations
1. **Fix Recall Bug** - Investigate why PRISM search yields near-zero recall
2. **Expose Entry Points** - Modify HNSW to accept custom entry candidates
3. **Refactor Trait** - Allow `search_mut` or use interior mutability for session state
4. **Cross-Session Learning** - Aggregate stats across users for global structure learning

---

## FUSION (Fast Unified Search)

### What It Does
Combines **LSH bucketing** (for coarse routing) with **Mini-Graphs** (for fine-grained search). Designed to fix the O(n) issues of SEER/LIM while maintaining high recall.

**Key Innovation:**
- **LSH Routing**: O(1) bucket selection
- **NSW Graphs**: Simple navigable small world graphs per bucket
- **Adaptive Probing**: Stops probing buckets when enough good candidates found

### ‚úÖ Upsides
| Advantage | Description |
|-----------|-------------|
| **High Recall** | 94% recall (best of the O(n)-fixing attempts) |
| **No Linear Scan** | LSH avoids O(n) candidate scoring |
| **Clean Architecture** | Well-separated components (Hasher, Graph, Reranker) |
| **Scalable** | Design scales better than linear scan for >100K vectors |

### ‚ùå Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **Small-Scale Speed** | üü† Medium | Slower than linear for <10K items due to overhead |
| **Uniform Data Issues** | üü° Medium | LSH acts poorly on uniform random benchmarks |
| **Scan Overhead** | üü° Medium | Probing multiple buckets can read more than needed |
| **Fixed Buckets** | üü° Medium | No dynamic splitting/merging of buckets |

### Recommendations
1. **Parallel Probing** - Search buckets concurrently
2. **Learned Routing** - Replace LSH with a learned classifier for bucket selection
3. **Hierarchical LSH** - Two-level hashing for very large datasets
4. **Optimize Overhead** - Reduce hashing/probing cost for small N

---

## VORTEX (Voronoi-Optimized Routing)

### What It Does
Solves the "Recall vs Speed" dilemma by replacing unreliable LSH or linear cluster scanning with **Graph-based Cluster Routing**.
Builds a small HNSW graph on K-Means centroids for O(log C) routing.

**Key Innovation:**
- **Centroid Graph**: Navigable graph on clusters
- **Density-Adaptive**: K-Means adapts to data distribution
- **Two-Stage Search**: Graph Route ‚Üí Bucket Scan

### ‚úÖ Upsides
| Advantage | Description |
|-----------|-------------|
| **Scalability** | O(log C) routing is extremely fast |
| **Recall** | Better than SWIFT due to density-aware clusters |
| **Build Time** | Faster than HNSW (only builds graph on centroids) |

### ‚ùå Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **Training Overhead** | üü† Medium | K-Means training can be slow for large N |
| **Memory** | üü° Medium | Duplicate storage (buckets + vectors) in current impl |

### Recommendations
1. **Subsample Training** - Speed up build
2. **Parallelize** - Assign vectors to buckets in parallel

---

## ARMI Algorithm (Adaptive Robust Multi-Modal Index)

### What It Does
Combines **multi-modal data support** (dense, sparse, audio), **distribution shift detection**, **adaptive parameter tuning**, **energy-aware execution**, and **deterministic search** into a unified indexing system.

**Key Innovations:**
- **Multi-Modal Graph**: Unified graph with cross-modal edges
- **Shift Detection**: Statistical monitoring (KS test, KL divergence) with automatic adaptation
- **RL-Based Tuning**: Reinforcement learning for optimal search parameters
- **Precision Scaling**: FP32 ‚Üí FP16 ‚Üí INT8 based on energy budgets
- **Deterministic**: Seeded RNG for reproducible results

### ‚úÖ Upsides
| Advantage | Description |
|-----------|-------------|
| **Comprehensive** | Addresses 5 research gaps simultaneously (1B, 5, 6A, 6B, 7A) |
| **Multi-Modal** | First algorithm with true multi-modal support (dense + sparse + audio) |
| **Robust** | Distribution shift detection and adaptation |
| **Adaptive** | RL-based parameter optimization |
| **Energy Efficient** | Precision scaling for energy-aware execution |
| **Deterministic** | Reproducible results via seeded RNG |
| **Unique Value** | Only algorithm combining all these capabilities |

### ‚ùå Downsides
| Issue | Severity | Description |
|-------|----------|-------------|
| **O(n¬≤) Build Time** | üî¥ Critical | Graph construction scans all nodes per insertion |
| **Incomplete Integration** | üü† Medium | Adaptive tuning and energy optimization not fully integrated |
| **Memory Overhead** | üü° Medium | Stores full multi-modal vectors + graph + statistics |
| **Complexity** | üü° Medium | Many components increase code complexity |
| **No Benchmarks** | üî¥ Critical | Cannot benchmark at scale due to build time |

### Testing Performed
- ‚úÖ Multi-modal tests (dense-only, hybrid, cross-modal)
- ‚úÖ Distribution shift detection
- ‚úÖ Adaptive parameter tuning
- ‚úÖ Deterministic behavior verification
- ‚úÖ Energy efficiency tests
- ‚ùå Standard benchmarks (timeout due to O(n¬≤) build)
- ‚ùå Large-scale testing (not possible)

### Recommendations
1. **Fix O(n¬≤) Build** - Add spatial index (LSH/KD-tree) for neighbor finding
2. **Complete Integration** - Integrate adaptive tuning and energy optimization into search
3. **Performance Optimization** - Parallel construction, caching, memory optimization
4. **Benchmarking** - Run small-scale benchmarks once build is fixed

---

## Baseline Algorithms

### HNSW (Hierarchical Navigable Small World)
- **Implementation**: Standard multi-layer navigable graph
- **Key parameters**: `m` (connections), `ef_construction`, `ef_search`
- **Status**: Fully implemented, serves as accuracy/speed baseline

### IVF (Inverted File Index)
- **Implementation**: K-means++ initialization, proper cluster training
- **Key innovation**: Separation of training vs search phase
- **Status**: Fully implemented with incremental inserts after build

### PQ (Product Quantization)
- **Implementation**: Subvector codebook learning
- **Compression**: Reduces memory ~4-8√ó while maintaining approximate distances
- **Status**: Fully implemented with K-means codebook training

---

## Related Documents

- **Benchmark scenarios**: See [README.md](./README.md#-running-benchmarks)
- **Research gaps**: See [research_gaps.md](./research_gaps.md) for full coverage

---

## Benchmark Results (Verified 2026-01-12)

> **Scenario**: `recall-baseline` ‚Äì 10,000 points, 64 dimensions, 256 queries, k=20, Euclidean

| Index | QPS | Avg Recall@20 | Build Time | Status |
|-------|-----|---------------|------------|--------|
| **Linear** | 1,230 | 100.0% | 634¬µs | ‚úÖ Baseline |
| **HNSW** | 33,299 | 1.0% | 220ms | ‚ö†Ô∏è Low recall (needs tuning) |
| **IVF** | 13,710 | 40.4% | 1.71s | ‚ö†Ô∏è Low recall (needs more probes) |
| **PQ** | 748 | 33.7% | 16.16s | ‚ö†Ô∏è Low recall (compression tradeoff) |
| **LIM** | 1,158 | 98.7% | 3.33s | ‚úÖ High recall, near-linear QPS |
| **Hybrid** | 1,256 | 100.0% | 557¬µs | ‚úÖ Perfect recall (falls back to linear) |
| **SEER** | 110 | 96.5% | 2.4ms | üî¥ 11√ó slower than linear |
| **SWIFT** | 15,884 | 6.0% | 73ms | üî¥ Very low recall |
| **PRISM** | 32,389 | 0.8% | 222ms | üî¥ Nearly zero recall |
| **NEXUS** | 2,329 | 14.6% | 8.39s | üî¥ Low recall, long build |
| **FUSION** | 527 | 94.0% | 553ms | ‚úÖ High recall, addresses O(n) issues |
| **ZENITH** | 2,113 | 94.82% | 4.85s | ‚úÖ Best balanced (zero config, see [zenith_analysis.md](./zenith_analysis.md)) |

### Key Observations

1. **ZENITH** is the best balanced: 94.82% recall, 2,113 QPS, zero config. See [zenith_analysis.md](./zenith_analysis.md).
2. **LIM**, **Hybrid**, and **FUSION** also achieve high recall (>90%)
3. **FUSION** addresses O(n) issues with LSH bucketing + mini-graphs
4. **HNSW** has catastrophically low recall (1%) with defaults; needs tuning
5. **SEER** has good recall but O(n) performance issue remains
6. **SWIFT**, **PRISM**, **NEXUS** have serious recall issues to investigate

---

## Performance Comparison Summary

| Metric | Linear | LIM | Hybrid | SEER | IVF | PQ | HNSW |
|--------|--------|-----|--------|------|-----|-------|------|
| **Search Complexity** | O(n) | O(clusters √ó k) | O(n) | O(n)* | O(probes √ó k) | O(n) | O(log n) |
| **Insert Complexity** | O(1) | O(clusters) | O(1) | O(1) | O(clusters)* | O(m)* | O(log n) |
| **Memory per Vector** | O(d) | O(d + 8) | O(d + sparse) | O(d + proj) | O(d) | O(m) | O(d + edges) |
| **Accuracy** | 100% | Approximate | Approximate | ~97% | Approximate | Approximate | Approximate |
| **Temporal Aware** | ‚ùå | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| **Hybrid Search** | ‚ùå | ‚ùå | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| **Learned** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |

*After training phase

---

## Next Steps

### Immediate Priorities
1. **Optimize FUSION speed** - Adaptive probing, parallel bucket search
2. **Fix LIM scale mismatch** - Normalize spatial distances
3. **Add sparse inverted index** to Hybrid
4. **Large-scale benchmarks** - 100K-1M vectors to validate FUSION scalability

### Research Extensions
1. **Learned FUSION routing** - Train model to predict best buckets
2. **Graph-accelerated Hybrid** - Combine HNSW with sparse lookup
3. **Temporal FUSION** - Add time decay post-search reranking
4. **Streaming support** - Handle continuous inserts without rebuilds

---

*See also: [SOTA_SUMMARY.md](./SOTA_SUMMARY.md), [zenith_analysis.md](./zenith_analysis.md), [fusion_analysis.md](./fusion_analysis.md), [seer_analysis.md](./seer_analysis.md), [lim_analysis.md](./lim_analysis.md), [research_gaps.md](./research_gaps.md)*

